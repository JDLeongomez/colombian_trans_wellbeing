---
title: "\\colorbox{pink}{Colombian trans wellbeing}"
subtitle: Code and analyses
author:
  - name: Maria Fernanda Reyes-Rodríguez \orcidlink{0000-0002-2645-5092}
    correspondence: true
    institute: andes
    email: m.reyes8@uniandes.edu.co
  - name: Juan David Leongómez \orcidlink{0000-0002-0092-6298}
    correspondence: false
    institute: codec
institute:
  - andes: "Department of Psychology, University of Los Andes, Bogota 111211, Colombia"
  - codec: "CODEC: Ciencias Cognitivas y del Comportamiento, Universidad El Bosque, Bogotá 110121, Colombia."
date: "`r Sys.setlocale('LC_TIME','en_GB.UTF-8');format(Sys.Date(),'%d %B, %Y')`"
output:
  bookdown::pdf_document2:
    citation_package: biblatex
    highlight: zenburn
    number_sections: yes
    keep_tex:  false
    toc: no
    pandoc_args:
      - '--lua-filter=lua/scholarly-metadata.lua'
      - '--lua-filter=lua/author-info-blocks.lua'
classoption: 
      - bookmarksnumbered
editor_options:
  chunk_output_type: console
geometry: margin=2cm
header-includes: 
  \usepackage{caption} 
  \usepackage{float} 
  \floatplacement{figure}{H} 
  \usepackage[utf8]{inputenc} 
  \usepackage{fancyhdr}
  \pagestyle{fancy} 
  \usepackage{hanging}
  \lhead{Reyes-Rodríguez \& Leongómez} 
  \rhead{\textit{\colorbox{pink}{Colombian trans wellbeing}}} 
  \renewcommand{\abstractname}{Description} 
  \usepackage[british]{babel}
  \usepackage{csquotes}
  \usepackage[style=apa,backend=biber]{biblatex}
  \DeclareLanguageMapping{british}{british-apa}
  \usepackage{hanging}
  \usepackage{amsthm,amssymb,amsfonts}
  \usepackage{tikz,lipsum,lmodern}
  \usepackage{multicol}
  \usepackage{orcidlink}
  \newcommand{\opensupplement}{\setcounter{table}{0}
    \renewcommand{\thetable}{S\arabic{table}} \setcounter{figure}{0}
    \renewcommand{\thefigure}{S\arabic{figure}}}
  \newcommand{\closesupplement}{\setcounter{table}{0}
    \renewcommand{\thetable}{\arabic{table}} \setcounter{figure}{0}
    \renewcommand{\thefigure}{\arabic{figure}}}
  \usepackage{multirow,booktabs,setspace}
  \DeclareCaptionLabelSeparator{point}{. }
  \DeclareCaptionLabelSeparator{point}{. }
  \captionsetup[table]{labelfont=bf,
    textfont=it,
    format=plain,
    labelsep=point,
    skip=5pt}
  \captionsetup[figure]{labelfont=bf,
    format=plain,
    justification=justified,
    singlelinecheck=false,
    labelsep=point,
    skip=5pt}
always_allow_html: yes
bibliography: bib/Bibliography.bib
urlcolor: blue
linkcolor: gray
citecolor: gray
link-citations: true
---

------------------------------------------------------------------------

```{=tex}
\begin{center}
\textbf{Description}
\end{center}

\par
\begingroup
\leftskip3em
\rightskip\leftskip
```

This document contains all code, and step by step explanations for all analyses, figures and tables (including supplementary figures and tables) for:

```{=latex}
\begin{hangparas}{.25in}{1}
Reyes-Rodríguez, M. F., \&  Leongómez, J. D. (in prep). \textit{\colorbox{pink}{Colombian trans wellbeing}}
\end{hangparas}
```

Data are available on the Open Science Framework (OSF): \colorbox{pink}{https://doi.org/10.XXXX/OSF.IO/XXXXX}. The analyses were designed by Maria Fernanda Reyes-Rodríguez and Juan David Leongómez. This document and its underlying code were created in R Markdown by Juan David Leongómez using R and \LaTeX, ensuring full reproducibility.

------------------------------------------------------------------------

```{=latex}
\par
\endgroup

{\hypersetup{hidelinks}
\setcounter{tocdepth}{6}
\tableofcontents
}
\opensupplement
```

```{r results = "hold", setup, include = FALSE}
library(knitr)
opts_chunk$set(fig.width = 12, fig.height = 6, fig.pos = "H")
options(knitr.kable.NA = " ")
opts_knit$set(eval.after = "fig.cap")
```

------------------------------------------------------------------------

# Preliminaries

## Load packages

This file was created using `knitr` [@knitrcit], mostly using `tidyverse` [@tidyversecit] syntax. As such, data wrangling was mainly done using packages such as `dplyr` [@dplyrcit], and most figures were created or modified using `ggplot2` [@ggplotcit]. Tables were created using `knitr::kable` and `kableExtra` [@kableExtracit].

Multi-model inference and model averaging was achieved using `MuMIn` [@MuMIncit], and model assumptions were performed using `performance` [@ludecke2021].

All packages used in this file can be directly installed from the Comprehensive R Archive Network ([CRAN](https://cran.r-project.org/)). For a complete list of packages used to create this file, and their versions, see section \@ref(session), at the end of the document.

```{r message = FALSE}
library(ltm)
library(psych)        # For statistical functions (e.g., Cronbach's alpha)
library(MuMIn)        # For model selection and averaging
library(performance)  # For model performance metrics
library(readr)        # For reading data files
library(scales)       # For percent formatting
library(knitr)
library(kableExtra)
library(car)
library(tidyverse)    # For data manipulation and piping
library(gtsummary)
library(Hmisc)
```

## Custom functions

### `pval.lev` and `pval.stars`

These functions take p-values and formats them, either in \LaTeX and highlighting significant p-values in bold and representing all in an appropriate level, or as stars.

```{r}
# Function to format p-values for LaTeX output, highlighting significant values in bold
pval.lev <- function(pvals) {
  ifelse(pvals < 0.0001, "\\textbf{< 0.0001}", # Highlight very small p-values
         ifelse(pvals < 0.001, "\\textbf{< 0.001}", # Bold p-values < 0.001
                ifelse(pvals < 0.05, paste0("\\textbf{", round(pvals, 4), "}"), # Bold p-values < 0.05
                       round(pvals, 2) # Round non-significant values to two decimal places
                )
         )
  )
}

# Function to add significance stars based on p-value thresholds
pval.stars <- function(pvals) {
  ifelse(pvals < 0.0001, "****", # Four stars for p < 0.0001
         ifelse(pvals < 0.001, "***", # Three stars for p < 0.001
                ifelse(pvals < 0.01, "**", # Two stars for p < 0.01
                       ifelse(pvals < 0.05, "*", NA) # One star for p < 0.05, NA otherwise
                )
         )
  )
}
```

### `corr.stars`

This function creates a correlation matrix, and displays significance (function `corr.stars` modified from <http://myowelt.blogspot.com/2008/04/beautiful-correlation-tables-in-r.html>).

```{r}
# Function to create a correlation matrix with significance levels in LaTeX format
corr.stars <- function(x) {
  require(Hmisc) # Load Hmisc package for correlation and p-value calculations
  x <- as.matrix(x) # Ensure input is a matrix
  R <- rcorr(x, type= "spearman")$r # Compute correlation coefficients
  p <- rcorr(x, type= "spearman")$P # Extract p-values for significance testing
  # Define symbols for significance levels, using LaTeX formatting for bold and stars
  mystars <- ifelse(p < .001, paste0("\\textbf{", round(R, 2), "***}"), # p < 0.001
    ifelse(p < .01, paste0("\\textbf{", round(R, 2), "**}"), # p < 0.01
      ifelse(p < .05, paste0("\\textbf{", round(R, 2), "*}"), # p < 0.05
        ifelse(p < .10, paste0(round(R, 2), "$^{\\dagger}$"), # p < 0.10 (trend level)
          format(round(R, 2), nsmall = 2) # Format non-significant values with two decimals
        )
      )
    )
  )
  # Construct a new matrix with correlation values and significance symbols
  Rnew <- matrix(mystars, ncol = ncol(x))
  # Ensure diagonal values remain the original correlation values (without significance symbols)
  diag(Rnew) <- paste(diag(R), " ", sep = "")
  # Assign row and column names for the formatted matrix
  rownames(Rnew) <- colnames(x)
  colnames(Rnew) <- paste(colnames(x), "", sep = "")
  # Remove the upper triangle of the matrix (including the diagonal) for a clean presentation
  Rnew <- as.matrix(Rnew)
  Rnew[upper.tri(Rnew, diag = TRUE)] <- ""
  # Convert to a data frame for better handling and remove the last empty column
  Rnew <- as.data.frame(Rnew)
  Rnew <- cbind(Rnew[1:length(Rnew) - 1])
  return(Rnew) # Return formatted correlation table
}
```

### `avg.model.anova`

XXXX

```{r}
avg.model.anova <- function(avg_model, data, response) {
  # Extract predictor names (remove intercept)
  selected_vars <- names(coef(avg_model))[-1]
  
  # Ensure selected_vars exist in the dataset
  selected_vars <- selected_vars[selected_vars %in% names(data)]
  
  if (length(selected_vars) == 0) {
    stop("No valid predictors found in model-averaged object.")
  }
  
  # Refit model using selected (averaged) predictors
  weighted_model <- lm(reformulate(selected_vars, response = response), data = data)
  
  # Compute Type III ANOVA
  anova_table <- weighted_model |>  
    Anova(type = "III") |>  
    broom::tidy() |> 
    mutate_at("term", str_replace_all, "_", " ") |> 
    mutate(df = paste(df, weighted_model$df.residual, sep = ", "),
           p.value = pval.lev(p.value)) |>
    filter(term != "Residuals") |> # Remove Residuals row
    kable(digits = 3,
          booktabs = TRUE,
          linesep = "",
          align = c("l", rep("c", 4)),
          caption = "XXXXXX",
          col.names = c("Term", "$SS_{term}$", "$df$", "$F$", "$p$"),
          escape = FALSE) |> 
    kable_styling(latex_options = "HOLD_position") |>
    footnote(
      general = "This ANOVA table was generated based on model-averaged estimates from 
      multimodel inference. The predictor terms included in the model were selected based on 
      their relative importance across candidate models ($\\\\Delta AICc$ < 2). 
      Sum of squares ($SS_{term}$) values correspond to Type III ANOVA calculations, 
      which test each term's contribution while controlling for all other predictors. 
      Degrees of freedom ($df$) are presented as term $df$ and residual $df$, where residual 
      $df$ reflects the remaining degrees of freedom in the model. The $F$ and $p$ values were
      computed from the refitted model using only the selected predictors.
      Significant effects are in bold.",
      threeparttable = TRUE, 
      footnote_as_chunk = TRUE, 
      escape = FALSE
    )
  return(anova_table)
}
```

### `avg.mod.plot`

XXXX 

```{r}
avg.mod.plot <- function(avg_mod) {
  # Extract model summary and transform into a tidy format
  x <- summary(avg_mod)$coefmat.full |> 
  as_tibble(rownames = "key") |>  # Convert row names to a "key" column
  bind_cols(
    confint(avg_mod, full = TRUE) |> as_tibble(),  # Add confidence intervals
    summary(avg_mod)$coef.nmod |> 
      as_tibble() |> 
      pivot_longer(cols = everything(), names_to = "model", values_to = "value")  # Gather number of models per term
  ) |> 
  mutate(
    avmod = deparse(substitute(avg_mod)) |> 
      factor(),  # Store model name as a factor
    value = value / max(value, na.rm = TRUE),  # Normalize 'value' column
    sig = pval.stars(`Pr(>|z|)`) |> 
      str_replace("\\.", "†"),  # Convert p-values into significance stars
    key = key |> 
      str_replace_all("Gender", "Gender: ") |> 
      str_replace_all("Housing", "Housing: ") |> 
      str_replace_all("_", " ")) 
  
  x <- x |> 
    mutate(key = factor(key, levels = as.character(unique(x$key))))
  
  # Get the number of averaged models
  nMods <- dim(avg_mod$msTable)[1]
  
  # Create the plot
  ggplot(x, aes(x = key, y = Estimate)) +
    # Add horizontal reference line at zero
    geom_hline(yintercept = 0, color = "grey") +
    # Add points sized and colored by importance
    geom_point(aes(size = value, color = value), alpha = 0.5) +
    # Add error bars (confidence intervals)
    geom_errorbar(aes(ymin = `2.5 %`, ymax = `97.5 %`),
                  colour = "black", width = 0.1) +
    # Add additional points for emphasis
    geom_point(size = 1) +
    # Apply theme and labels
    theme_bw() +
    labs(x = NULL, y = "Estimate") +
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
    # Scale for importance (value) size
    scale_size_continuous(range = c(2, 8),
                          breaks = seq(0, 1, by = 0.2)) +
    # Legends for size and color
    guides(size = guide_legend(title = "Importance"),
           color = guide_legend(title = "Importance")) +
    # Ensure x-axis labels remain in the correct order
    scale_x_discrete(labels = levels(x$key),
                     expand = c(0, 0.5)) +
    # Use plasma color scale, reversed so darker colors represent higher values
    scale_colour_viridis_c(option = "plasma", direction = -1) +
    # Add significance labels next to points
    geom_text(aes(label = sig), y = x$`97.5 %`, vjust = -0.4) +
    # Add annotation indicating the number of averaged models
    geom_text(aes(x = Inf, y = -Inf,
                  label = paste("Models averaged = ", nMods)),
              #fontface = "italic",
              size = 3,
              hjust = 1.1,
              vjust = -0.5,
              inherit.aes = FALSE)
}
```

## Load data

Load raw CSV data

```{r message = FALSE}
data_RAW <- read_csv("data/data.csv")
```

### Define PANAS Subscales (Positive & Negative Affect)

XXXXXXX 

```{r message = FALSE}
# List of PANAS Positive Affect (PANAS_P) items
PANAS_P <- c("PANASB_1", "PANASB_3", "PANASB_5", "PANASB_9", 
             "PANASB_10", "PANASB_12", "PANASB_14", "PANASB_16", 
             "PANASB_17", "PANASB_19")

# List of PANAS Negative Affect (PANAS_N) items
PANAS_N <- c("PANASB_2", "PANASB_4", "PANASB_6", "PANASB_7",
             "PANASB_8", "PANASB_11", "PANASB_13", "PANASB_15",
             "PANASB_18", "PANASB_20")
```

## Internal consistency

### Calculate Cronbach's Alpha for Different Scales

To measure the internal consistency of these tests, we used standardized Cronbach's alpha ($\alpha$ or Tau-equivalent reliability: $\rho_{T}$) coefficients, using the function `cronbach.alpha` from the package `ltm` [@lmtcit].

```{r cronbach-alpha, message = FALSE, warning = FALSE}
# Compute Cronbach's alpha for the Self-Efficacy (EAG) scale
alpha_EAG <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA (missing values)
  select(starts_with("EAG_")) |>  # Select all columns starting with "EAG_"
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)  # Compute Cronbach’s alpha

# Compute Cronbach's alpha for the Life-Satisfaction (SWLS) scale
alpha_SWLS <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(starts_with("SWLS_")) |>  # Select all columns starting with "SWLS_"
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)

# Compute Cronbach's alpha for the Resilience (EBR) scale
alpha_EBR <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(starts_with("EBR_")) |>  # Select all columns starting with "EBR_"
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)

# Compute Cronbach's alpha for the Depression (EBD) scale (after recoding responses)
alpha_EBD <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(starts_with("EBD_")) |>  # Select all columns starting with "EBD_"
  mutate(across(everything(), ~ ifelse(is.na(.x), NA, .x - 1))) |>  # Adjust values
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)

# Compute Cronbach's alpha for the Social Support (MOS2) scale
alpha_MOS2 <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(starts_with("MOS2_")) |>  # Select all columns starting with "MOS2_"
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)

# Compute Cronbach's alpha for PANAS Positive Affect (PANAS_P)
alpha_PANAS_P <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(all_of(PANAS_P)) |>  # Select PANAS_P variables
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)

# Compute Cronbach's alpha for PANAS Negative Affect (PANAS_N)
alpha_PANAS_N <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(all_of(PANAS_N)) |>  # Select PANAS_N variables
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)

# Compute Cronbach's alpha for Community Cohesion (PCPS3) scale
alpha_PCPS3 <- data_RAW |> 
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>  # Replace 99 with NA
  select(starts_with("PCPS3_")) |>  # Select all columns starting with "PCPS3_"
  drop_na() |>
  cronbach.alpha(CI = TRUE, standardized = TRUE)
```

### Table \@ref(tab:tab-cronbach-alpha). Internal consistency of measured scales

The internal consistency of the measured scales was generally strong, with Cronbach’s $\alpha$ values ranging from `r round(min(alpha_EAG$alpha, alpha_SWLS$alpha, alpha_EBR$alpha, alpha_EBD$alpha, alpha_MOS2$alpha, alpha_PANAS_P$alpha, alpha_PANAS_N$alpha, alpha_PCPS3$alpha), 3)` to `r round(max(alpha_EAG$alpha, alpha_SWLS$alpha, alpha_EBR$alpha, alpha_EBD$alpha, alpha_MOS2$alpha, alpha_PANAS_P$alpha, alpha_PANAS_N$alpha, alpha_PCPS3$alpha), 3)`. In particular, the Social Support (MOS2) and Self-Efficacy (EAG) scales exhibited excellent internal consistency, while the Depression (EBD) and Community Cohesion (PCPS3) scales had acceptable reliability, suggesting a slightly lower but still adequate level of internal consistency.

```{r tab-cronbach-alpha, message = FALSE, warning = FALSE}
tibble(
  Scale = c("Self-Efficacy$^1$",
            "Life-Satisfaction$^2$",
            "Resilience$^3$",
            "Depression$^4$",
            "Social Support$^5$",
            "PANAS Positive$^6$",
            "PANAS Negative$^6$",
            "Community Cohesion$^x$"),
  p = c(alpha_EAG$p, alpha_SWLS$p, alpha_EBR$p, alpha_EBD$p, alpha_MOS2$p,
        alpha_PANAS_P$p, alpha_PANAS_N$p, alpha_PCPS3$p),
  n = c(alpha_EAG$n, alpha_SWLS$n, alpha_EBR$n, alpha_EBD$n, alpha_MOS2$n,
        alpha_PANAS_P$n, alpha_PANAS_N$n, alpha_PCPS3$n),
  alpha = c(alpha_EAG$alpha, alpha_SWLS$alpha, alpha_EBR$alpha, alpha_EBD$alpha, 
            alpha_MOS2$alpha, alpha_PANAS_P$alpha, alpha_PANAS_N$alpha, alpha_PCPS3$alpha),
  ci2.5 = c(alpha_EAG$ci[1], alpha_SWLS$ci[1], alpha_EBR$ci[1], alpha_EBD$ci[1], 
            alpha_MOS2$ci[1], alpha_PANAS_P$ci[1], alpha_PANAS_N$ci[1], alpha_PCPS3$ci[1]),
  ci97.5 = c(alpha_EAG$ci[2], alpha_SWLS$ci[2], alpha_EBR$ci[2], alpha_EBD$ci[2], 
             alpha_MOS2$ci[2], alpha_PANAS_P$ci[2], alpha_PANAS_N$ci[2], alpha_PCPS3$ci[2])) |>
  mutate(across(starts_with("ci"), round, 3)) |> 
  unite(col = "CI", ci2.5:ci97.5, sep = " — ") |> 
  kable(digits = 3,
        booktabs = TRUE,
        linesep = "",
        align = c("l", rep("c", 4)),
        caption = "Internal consistency of measured scales",
        col.names = c("Variable", "Items", "$n$", "$\\alpha$", "$95\\% CI$"),
        escape = FALSE) |> 
  kable_styling(latex_options = "HOLD_position") |>
  footnote(
    general = "95\\\\% confidence intervals were calculated with 1,000 bootstrap samples.
           Standardized Cronbach's alpha ($\\\\alpha$) coefficients were computed.
           $^1$\\\\cite{EAG};
           $^2$\\\\cite{SWLS};
           $^3$\\\\cite{EBR};
           $^4$\\\\cite{EBD};
           $^5$\\\\cite{MOS};
           $^6$\\\\cite{PANAS}.",
    threeparttable = TRUE, footnote_as_chunk = TRUE, escape = FALSE
  )
```

# Data Preprocessing

## Renaming, recoding, and filtering

XXXX

```{r message = FALSE}
data <- data_RAW |>
  # Rename columns to meaningful names
  rename(
    Age = SD1,
    City = SD2,
    Gender = SD3,
    Sexualientation = SD4,
    Sex = SD5,
    Ethnicity = SD6,
    Farmer = SD7,
    Marital_Status = SD8,
    SES = SD9, # Socioeconomic Status
    Education = SD10,
    Children = SD11,
    Housing = SD12,
    Cohabitant = SD13,
    Monthly_Income = SD14,
    Income_Source = SD15,
    Employment = SD16,
    Job = SD17,
    # Disabilities and difficulties
    Hearing_Difficulties = SD18_1,
    Speaking_Difficulties = SD18_2,
    Seeing_Difficulties = SD18_3,
    Moving_Difficulties = SD18_4,
    Grabing_Difficulties = SD18_5,
    Understanding_Difficulties = SD18_6,
    Interacting_Difficulties = SD18_7,
    # Lifetime Prevalence (LP) of substance use
    LP_Alcohol = SD19_1_A,
    LP_Cigarette = SD19_2_A,
    LP_Cannabis = SD19_3_A,
    LP_Cocaine = SD19_4_A,
    LP_Basuco = SD19_5_A,
    LP_Inhalant = SD19_6_A,
    LP_Ecstasy = SD19_7_A,
    LP_Psilocybin = SD19_8_A,
    LP_LSD = SD19_9_A,
    LP_Tranquilizer = SD19_10_A,
    LP_Popper = SD19_11_A,
    LP_Anfetamines = SD19_12_A,
    LP_Heroine = SD19_13_A,
    # Last Month (LM) substance use
    LM_Alcohol = SD19_1_B,
    LM_Cigarette = SD19_2_B,
    LM_Cannabis = SD19_3_B,
    LM_Cocaine = SD19_4_B,
    LM_Basuco = SD19_5_B,
    LM_Inhalant = SD19_6_B,
    LM_Ecstasy = SD19_7_B,
    LM_Psilocybin = SD19_8_B,
    LM_LSD = SD19_9_B,
    LM_TRAN = SD19_10_B,
    LM_Popper = SD19_11_B,
    LM_Anfetamines = SD19_12_B,
    LM_Heroine = SD19_13_B,
    # Last Week (LW) substance use
    LW_Alcohol = SD19_1_C,
    LW_Cigarette = SD19_2_C,
    LW_Cannabis = SD19_3_C,
    LW_Cocaine = SD19_4_C,
    LW_Basuco = SD19_5_C,
    LW_Inhalant = SD19_6_C,
    LW_Ecstasy = SD19_7_C,
    LW_Psilocybin = SD19_8_C,
    LW_LSD = SD19_9_C,
    LW_Tranquilizer = SD19_10_C,
    LW_Popper = SD19_11_C,
    LW_Anfetamines = SD19_12_C,
    LW_Heroine = SD19_13_C,
    Health = SD20_1,
    # Health and other variables
    Illness = SD21,
    Disease_Other = SD22_13_TEXT,
    PCPS1_4_Other = PCPS1_4_texto,
    eed1_7_Other = EED1_7_TEXT
  ) |>
  # Replace character "99" with NA for missing values
  mutate(across(where(is.character), ~ na_if(., "99"))) |>
  # Replace numeric 99 with NA for missing values
  mutate(across(where(is.numeric), ~ na_if(., 99))) |>
  # Recode gender categories into descriptive labels
  mutate(
    Gender = recode(
      Gender,
      "1" = "Male",
      "2" = "Female",
      "3" = "Androgynous",
      "4" = "Trans woman",
      "5" = "Trans man",
      "6" = "Trans feminine",
      "7" = "Trans masculine",
      "8" = "Queer",
      "9" = "Non-binary",
      "10" = "Don't know",
      "11" = "Other"
    )) |>
  # Create a broader Gender category for analysis
  mutate(Gender = if_else(Gender %in% c(
    "Woman", "Trans feminine", "Transexual", "Travesti", "Trans woman"),
    "Trans woman",  
    if_else(Gender %in% c("Man", "Trans masculine", "Trans man"),
            "Trans man",
            "Non-binary")
  )) |>
  # Recode housing categories into descriptive labels
  mutate(
    Housing = recode(
      Housing,
      "1" = "Home-owner",
      "2" = "Renting (entire home)",
      "3" = "Living with family",
      "4" = "Shared rental (room)",
      "5" = "Without permanent housing"
    )
  ) |>
  # Recode substance use responses from text to binary (1 = Yes, 0 = No)
  mutate_at(
    c(
      "LP_Alcohol",
      "LP_Cigarette",
      "LP_Cannabis",
      "LP_Cocaine",
      "LP_Basuco",
      "LP_Inhalant",
      "LP_Ecstasy",
      "LP_Psilocybin",
      "LP_LSD",
      "LP_Tranquilizer",
      "LP_Popper",
      "LP_Anfetamines",
      "LP_Heroine",
      "LM_Alcohol",
      "LM_Cigarette",
      "LM_Cannabis",
      "LM_Cocaine",
      "LM_Basuco",
      "LM_Inhalant",
      "LM_Ecstasy",
      "LM_Psilocybin",
      "LM_LSD",
      "LM_TRAN",
      "LM_Popper",
      "LM_Anfetamines",
      "LM_Heroine",
      "LW_Alcohol",
      "LW_Cigarette",
      "LW_Cannabis",
      "LW_Cocaine",
      "LW_Basuco",
      "LW_Inhalant",
      "LW_Ecstasy",
      "LW_Psilocybin",
      "LW_LSD",
      "LW_Tranquilizer",
      "LW_Popper",
      "LW_Anfetamines",
      "LW_Heroine"
    ),
    ~ recode(.x, "1" = 1, "2" = 0)
  ) |>
  # Select only relevant variables
  select(
    -c(
      Codigo,
      ends_with("_TEXT"),
      Sexualientation,
      ends_with("_texto")
    )
  ) |>
  # Recode ethnicity, farmer status, marital status, SES, and education
  mutate(
    Ethnicity = recode(
      Ethnicity,
      "1" = "Indigenous",
      "2" = "Rrom",
      "3" = "Afro-Colombian",
      "4" = "Afro-Colombian",
      "5" = "Afro-Colombian",
      "6" = "Afro-Colombian",
      "7" = "Afro-Colombian",
      "8" = "Afro-Colombian",
      "9" = "None"
    )
  ) |>
  mutate(Farmer = recode(
    Farmer,
    "1" = "Yes",
    "2" = "No",
    "5" = NA_character_
  )) |>
  mutate(
    Marital_Status = recode(
      Marital_Status,
      "1" = "Married",
      "2" = "Single",
      "3" = "Widow/er",
      "4" = "Divorced",
      "5" = "Civil union",
      "6" = "Stable relationship"
    )
  ) |>
  mutate(
    SES = recode_factor(
      SES,
      "1" = "Low",
      "2" = "Low",
      "7" = "Low",
      "3" = "Middle-low",
      "4" = "Middle-high",
      "5" = "High",
      "6" = "High"
    )
  ) |>
  mutate(
    Education = recode_factor(
      Education,
      "1" = "No studies, illiterate",
      "2" = "No studies, literate",
      "3" = "Primary school (unfinished)",
      "4" = "Primary school",
      "5" = "Secondary school (unfinished)",
      "6" = "Secondary school",
      "7" = "Technical degree",
      "8" = "University (unfinished)",
      "9" = "University",
      "10" = "Postgraduate studies"
    )
  ) |>
  mutate(across(c(SD22_1:SD22_13, 
                  EED1_1,
                  EED1_2,
                  EED1_3,
                  EED1_4,
                  EED1_5,
                  EED1_6,
                  EED1_7,
                  EED2_1:EED2_5),
                ~ as.numeric(
                  recode(
                    as.character(.x),
                    "1" = "1",
                    "2" = "0",
                    .default = NA_character_,
                    .missing = NA_character_
                  )
                ))) |>
  # Convert disability variables to binary (1 = Has difficulty, 0 = No difficulty)
  mutate(across(
    ends_with("_Difficulties"),
    ~ case_when(.x == 99 ~ NA_real_, is.na(.x) ~ NA_real_, .x == 4 ~ 1, TRUE ~ 0)
  )) |>
  # Create a new variable 'difficulty_dichotomous' to indicate whether a person has
  # any difficulties across different categories
  mutate(Difficulty_Dichotomous = if_else(
    # If any of the difficulties variables (e.g., Hearing_Difficulties, Speaking_Difficulties, etc.) are NA
    rowSums(across(ends_with("_Difficulties"), ~ is.na(.))) > 0,
    NA_real_, # Assign NA if any difficulty is missing
    # Otherwise, check if all seven difficulties are marked as '1' (indicating impairment)
    if_else(rowSums(across(ends_with("_Difficulties"), ~ . == 1)) == 7,
            1, # Assign 1 if the person has all seven difficulties
            0 # Assign 0 otherwise
    )
  )) |>
  # Recode PCPS1_1 to PCPS1_5: Convert 1 to 1 (yes) and 2 to 0 (no), with NA for other values
  mutate(across(PCPS1_1:PCPS1_5, ~ case_when(
    . == 1 ~ 1, # Yes
    . == 2 ~ 0, # No
    TRUE ~ NA_real_ # Missing or other values
  ))) |>
  # Recode PCPS2_1 to PCPS2_5: Convert 1 to 0 (no engagement), and 2-5 to 1 (some engagement)
  mutate(across(PCPS2_1:PCPS2_5, ~ case_when(
    . == 1 ~ 0, # No engagement
    . %in% 2:5 ~ 1, # Some engagement
    TRUE ~ NA_real_ # Missing or other values
  ))) |>
  
  mutate(across(starts_with("EBD_"), ~ ifelse(is.na(.x), NA, .x - 1))) |> 
  # Compute aggregate variables summarizing different aspects
  mutate(
    # Count the number of substances used in the last month
    Polyconsumption_Month = rowSums(across(LM_Alcohol:LM_Heroine, ~.), na.rm = TRUE),
    # Count the number of reported diseases or health conditions
    Disease_Burden = rowSums(across(SD22_1:SD22_13, ~.), na.rm = TRUE),
    # Count the number of group memberships (sum of binary indicators)
    Group_Membership = rowSums(across(PCPS1_1:PCPS1_5, ~.), na.rm = TRUE),
    # Count the number of community engagement activities
    Community_Engagement = rowSums(across(PCPS2_1:PCPS2_5, ~.), na.rm = TRUE),
    # The following line is commented out: it would sum discrimination experiences
    Discrimination = rowSums(across(EED1_1:EED1_7, ~.), na.rm = TRUE),
    Discrimination = ifelse(Discrimination >= 1, 1, 0),
    
    Self_Efficacy = if_else(rowSums(!is.na(across(starts_with("EAG_")))) >= 5,
                            rowMeans(across(starts_with("EAG_")), na.rm = TRUE),
                            NA_real_),
    
    Life_Satisfaction = if_else(rowSums(!is.na(across(starts_with("SWLS_")))) >= 3,
                                rowMeans(across(starts_with("SWLS_")), na.rm = TRUE),
                                NA_real_),
    
    Resilience = if_else(rowSums(!is.na(across(starts_with("EBR_")))) >= 3,
                         rowMeans(across(starts_with("EBR_")), na.rm = TRUE),
                         NA_real_),
    
    Depression = if_else(rowSums(!is.na(across(starts_with("EBD_")))) >= 6,
                         rowMeans(across(starts_with("EBD_")), na.rm = TRUE),
                         NA_real_),
    
    Social_Support = if_else(rowSums(!is.na(across(starts_with("MOS2_")))) >= 10,
                             rowMeans(across(starts_with("MOS2_")), na.rm = TRUE),
                             NA_real_),
    
    Positive_Affect = if_else(rowSums(!is.na(across(all_of(PANAS_P)))) >= 8,
                              rowMeans(across(all_of(PANAS_P)), na.rm = TRUE),
                              NA_real_),
    
    Negative_Affect = if_else(rowSums(!is.na(across(all_of(PANAS_N)))) >= 9,
                              rowMeans(across(all_of(PANAS_N)), na.rm = TRUE),
                              NA_real_),                          
    
    Community_Cohesion = if_else(rowSums(!is.na(across(starts_with("PCPS3_")))) >= 2,
                                 rowMeans(across(starts_with("PCPS3_")), na.rm = TRUE),
                                 NA_real_),
    
  ) |>
  select(Age, Gender, Ethnicity, Marital_Status, SES, Education, Housing, 
         Health, Polyconsumption_Month:Community_Cohesion) |>
  # Convert categorical variables Housing to Job into factors
  mutate(Housing = as.factor(Housing)) |>
  # Convert all remaining character variables to factors
  mutate_if(is.character, as.factor) |> 
  
  filter(Age >= 18)
```

## Missing Data

Create a summary of missing data for each variable in the final dataset

```{r message = FALSE}
Missing_data <- data |>
  # Summarize across all columns, counting the number of NA values in each column
  summarise(across(everything(), ~ sum(is.na(.)))) |>
  # Convert the summary from wide format (one row, many columns) to long format
  pivot_longer(
    everything(),               # Select all columns
    names_to = "Variable",      # Store column names in a new variable "Variable"
    values_to = "NA_count"      # Store the count of NAs in a new variable "NA_count"
  ) |> 
  # Compute the proportion of missing values for each variable
  mutate(Proportion = NA_count / dim(data)[1])  # Divide NA count by total number of rows in 'data'
```

### Fig. \@ref(fig:fig-missing-data). Proportion of missing data

To apply multi-model inference techniques such as dredge and model averaging, models must be fitted with complete data. Therefore, assessing the proportion of missing data per variable was crucial. While excessive missingness could lead to unreliable models, imputing missing values might reduce data credibility. Since no variable had an unacceptably high proportion of missing data, we opted not to impute missing values.

```{r fig-missing-data, fig.height = 4, fig.width = 6, fig.cap = "Proportion of missing data per variable. Variables are ordered from highest to lowest proportion of missing values. The color gradient indicates the proportion of missingness, with darker shades representing higher percentages."}
Missing_data |> 
  mutate_at("Variable", str_replace_all, "_", " ") |> 
  ggplot(aes(
    x = fct_reorder(Variable, Proportion, .desc = TRUE), # Reorder variables from highest to lowest missingness
    y = Proportion,
    fill = Proportion  # Use fill color to indicate proportion of missing data
  )) +
  geom_col() +  # Create bar plot
  # Add percentage labels on top of bars
  geom_text(aes(label = percent(Proportion, accuracy = 1)),  
            vjust = -0.5, size = 2) +
  # Apply color gradient: Green (low missing data) → Yellow (moderate) → Red (high missing data)
  scale_fill_viridis_c(
    option = "plasma",  # Define color range
    direction = -1,  # Reverse the color scale
    labels = percent_format(accuracy = 1)  # Convert legend values to percentage format
  ) + 
  # Convert Y-axis (proportion of missing data) to a percentage scale
  scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
  # Add axis labels
  labs(
    y = "Percentage of Missing Data",  # Label for Y-axis
    x = "Variable"  # Label for X-axis
  ) +
  # Use a minimal theme for a cleaner visual appearance
  theme_minimal() +
  # Rotate X-axis labels for better readability
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

# Descriptives

## Socio-demographic characteristics by gender

```{r}
data |> 
  select(Age:Housing) |> 
  rename_with(~ gsub("_", " ", .x)) |> 
  tbl_summary(by = Gender) |> 
  add_n() |> # add column with total number of non-missing observations
  bold_labels() |> 
  remove_footnote_header(columns = all_stat_cols()) |> 
  as_kable_extra(format = "latex", 
                 linesep = "",
                 booktabs = TRUE, 
                 caption = "Sociodemographic characteristics of the study participants 
                 by gender identity") |> 
  kable_styling(latex_options = c("scale_down", "HOLD_position")) |> 
  footnote(general = "Values are presented as median (Q1, Q3) for Age and frequency (\\\\%) 
           for the remaining variables.",
           threeparttable = TRUE,
           footnote_as_chunk = TRUE,
           escape = FALSE)
```

## Measured variables by gender

```{r}
data |> 
  select(Gender, Polyconsumption_Month:Community_Cohesion) |> 
  rename_with(~ gsub("_", " ", .x)) |> 
  tbl_summary(by = Gender) |> 
  add_n() |> # add column with total number of non-missing observations
  bold_labels() |> 
  remove_footnote_header(columns = all_stat_cols()) |> 
  as_kable_extra(format = "latex", 
                 linesep = "",
                 booktabs = TRUE, 
                 caption = "Other sociodemographic, health, and psychosocial characteristics 
                 of the study participants by gender identity") |> 
  kable_styling(latex_options = c("scale_down", "HOLD_position")) |> 
  footnote(general = "Continuous variables are presented as median (Q1, Q3), while categorical
           variables are shown as frequency (\\\\%).",
           threeparttable = TRUE,
           footnote_as_chunk = TRUE,
           escape = FALSE)
```

## Correlations

```{r}
# Compute correlations for all participants combined
dat.corr.ALL <- data |>
  select(Age, Polyconsumption_Month:Community_Cohesion) |> # Select numeric variables
  corr.stars() |>
  tail(-1) |> 
  rownames_to_column(var = " ")

# Compute correlations for non-binary participants
dat.corr.NB <- data |>
  filter(Gender == "Non-binary") |> # Select only non binary
  select(Age, Polyconsumption_Month:Community_Cohesion) |> # Select numeric variables
  corr.stars() |> # Compute correlation matrix with significance stars
  tail(-1) |> 
  rownames_to_column(var = " ") # Move row names to a column

# Compute correlations for trans men
dat.corr.TM <- data |>
  filter(Gender == "Trans man") |> # Select only trans men
  select(Age, Polyconsumption_Month:Community_Cohesion) |> # Select numeric variables
  corr.stars() |>
  tail(-1) |> 
  rownames_to_column(var = " ")

# Compute correlations for trans women
dat.corr.TW <- data |>
  filter(Gender == "Trans woman") |> # Select only trans women
  select(Age, Polyconsumption_Month:Community_Cohesion) |> # Select numeric variables
  corr.stars() |>
  tail(-1) |> 
  rownames_to_column(var = " ")

# Format and combine the correlation tables
bind_rows(dat.corr.ALL, dat.corr.NB, dat.corr.TM, dat.corr.TW) |>
  rename_with(~ gsub("_", " ", .x)) |> 
  mutate_at(" ", str_replace_all, "_", " ") |> 
  kable(
    digits = 2, booktabs = TRUE, 
    align = c("l", rep("c", 13)), 
    linesep = "",
    caption = "Correlations between measured variables", escape = FALSE
  ) |>
  # Add grouped row labels for each participant group
  pack_rows("All participants",
    start_row = 1, end_row = 13, bold = FALSE,
    background = "lightgray"
  ) |>
  pack_rows("Non binary",
    start_row = 14, end_row = 26, bold = FALSE,
    background = "lightgray"
  ) |>
  pack_rows("Trans men",
    start_row = 27, end_row = 39, bold = FALSE,
    background = "lightgray"
  ) |>
    pack_rows("Trans women",
    start_row = 40, end_row = 52, bold = FALSE,
    background = "lightgray"
  ) |>
  # Apply LaTeX styling
  kable_styling(latex_options = c("HOLD_position", "scale_down")) |>
  column_spec(2:10, width = "2.2cm") |> # Adjust column widths
  # Add footnote explaining correlation significance levels
  footnote(
    general = paste0(
      "Values represent Spearman correlation coefficients ($\\\\rho$). ",
      "For significance, $^{\\\\dagger}p$ < 0.1, *$p$ < 0.05, ",
      "**$p$ < 0.01, ***$p$ < 0.001. ",
      "Significant correlations are in bold."
    ),
    threeparttable = TRUE, footnote_as_chunk = TRUE, escape = FALSE
  ) |>
  landscape() # Rotate table for better readability in LaTeX
```

# Multi-model inference

XXXX

## Life Satisfaction model

XXXX

```{r}
dat_LS <- data |> 
  select(Life_Satisfaction, Age, Gender, Ethnicity, Marital_Status, Education, Housing, 
         Self_Efficacy, Community_Cohesion, Depression, Social_Support, Polyconsumption_Month,
         Disease_Burden, Discrimination, Group_Membership, Community_Engagement)|> 
  drop_na()

global_LS <- lm(Life_Satisfaction ~ Age + Gender + Ethnicity + Marital_Status + Education +
                  Housing + Self_Efficacy + Community_Cohesion + Depression + 
                  Social_Support + Polyconsumption_Month + Disease_Burden + Discrimination +
                  Group_Membership + Community_Engagement,
                data = dat_LS,
                na.action = "na.fail")
```

### Dredge

XXXX

```{r message = FALSE, warning = FALSE, cache = TRUE}
dr_LS <- dredge(global_LS, 
               trace = 2 #para ver barra de progreso
)
```

#### Fig. \@ref(fig:fig-dredge-LS). Dredge results of the Life Satisfaction model

XXXX

```{r fig-dredge-LS, fig.height = 8, fig.cap = "XXXX."}
plot(dr_LS[1:100,])
```

### Average model

XXXX

```{r}
avg_LS <- model.avg(dr_LS, subset = delta < 2, fit = TRUE)

model_avg = avg_LS

# Extract coefficients and format as a tidy tibble
coef_df <- as.data.frame(summary(model_avg)$coefmat.full) |> 
  rownames_to_column(var = "Variable") |>  # Move row names to a column
  rename(
    estimate = Estimate,
    std_error = `Std. Error`,
    adj_se = `Adjusted SE`,  # Adjusted standard error from model averaging
    z_value = `z value`,
    p_value = `Pr(>|z|)`
  )

library(gt)

coef_df |> 
  gt() |> 
  tab_header(title = "Model-Averaged Estimates") |> 
  fmt_number(columns = c(estimate, std_error, adj_se, z_value, p_value), decimals = 3) |> 
  cols_label(
    Variable = "**Variable**",
    estimate = "**Estimate**",
    std_error = "**SE**",
    adj_se = "**Adj. SE**",
    z_value = "**Z Value**",
    p_value = "**p-value**"
  ) |> 
  tab_options(table.font.size = "small")
```

#### Table \@ref(tab:tab-avg-LS). XXXX

XXXX

```{r tab-avg-LS, message = FALSE, warning = FALSE}
avg.model.anova(avg_LS, data = dat_LS, response = "Life_Satisfaction")
```

#### Fig. \@ref(fig:fig-avg-LS). Dredge results of the Life Satisfaction model

XXXX

```{r fig-avg-LS, warning = FALSE, fig.height = 4, fig.width = 6, fig.cap = "XXXX."}
avg.mod.plot(avg_LS)
```



-----------------------------------------------------------------------

# Session info (for reproducibility) {#session}

```{r results = "hold"}
# Display session information for reproducibility
# - Uses `pander()` for better formatting
# - `locale = FALSE` to exclude locale-specific info (reduces clutter)
library(pander)
pander(sessionInfo(), locale = FALSE)
```

------------------------------------------------------------------------

# Supplementary references {#refs}

\begin{multicols}{2}
\AtNextBibliography{\footnotesize}
\printbibliography[heading=none]
\normalsize
\end{multicols}

\def\printbibliography{}